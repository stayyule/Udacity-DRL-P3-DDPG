{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: https://user-images.githubusercontent.com/10624937/42135623-e770e354-7d12-11e8-998d-29fc74429ca2.gif \"Trained Agent\"\n",
    "\n",
    "\n",
    "# Collaboration and Competition Report\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. About the problem\n",
    "\n",
    "Reinforcement learning (RL) has recently been applied to solve challenging problems, from game\n",
    "playing to robotics. In industrial applications, RL is emerging as a practical component\n",
    "in large scale systems such as data center cooling. Most of the successes of RL have been in\n",
    "single agent domains, where modelling or predicting the behaviour of other actors in the environment\n",
    "is largely unnecessary.\n",
    "\n",
    "However, there are a number of important applications that involve interaction between multiple\n",
    "agents, where emergent behavior and complexity arise from agents co-evolving together. For example,\n",
    "multi-robot control, the discovery of communication and language, multiplayer games, \n",
    "and the analysis of social dilemmas  all operate in a multi-agent domain. Related problems,\n",
    "such as variants of hierarchical reinforcement learning can also be seen as a multi-agent system,\n",
    "with multiple levels of hierarchy being equivalent to multiple agents. Additionally, multi-agent\n",
    "self-play has recently been shown to be a useful training paradigm. Successfully scaling RL\n",
    "to environments with multiple agents is crucial to building artificially intelligent systems that can\n",
    "productively interact with humans and each other.\n",
    "\n",
    "For this project, you will work with the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment.\n",
    "\n",
    "![Trained Agent][image1]\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation.  Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,\n",
    "\n",
    "- After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "- This yields a single **score** for each episode.\n",
    "\n",
    "The environment is considered solved, when the average (over 100 episodes) of those **scores** is at least +0.5.\n",
    "\n",
    "#### Explore the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unity ml-agents path\n",
    "import sys\n",
    "sys.path.append(\"../python/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"../Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Benchmark\n",
    "\n",
    "Benchmark for the agent is using random actions, which can get average lower than 0.05 if you get a good luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.10000000149011612\n",
      "Score (max over agents) from episode 2: 0.09000000171363354\n",
      "Score (max over agents) from episode 3: 0.10000000149011612\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    t = 0\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        t += 1\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Solution\n",
    "\n",
    "#### MADDPG\n",
    "Multi-agent DDPG is the framework of centralized training with\n",
    "decentralized execution. Thus, we allow the policies to use extra information to ease training, so\n",
    "long as this information is not used at test time. It is unnatural to do this with Q-learning, as the Q\n",
    "function generally cannot contain different information at training and test time. Thus, we propose\n",
    "a simple extension of actor-critic policy gradient methods where the critic is augmented with extra\n",
    "information about the policies of other agents.\n",
    "\n",
    "![maddpg](./pic/maddpg.png)\n",
    "\n",
    "The difference between basic ddpg and maddpg is that critic network will include inputs from other agents as input to train and evaluate.\n",
    "\n",
    "\n",
    "<img src=\"./pic/psuedo.png\"  height=\"500\" width=\"800\" align='left'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MADDPG model\n",
    "\n",
    "Basic agent takes DDPG codes from https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum as reference with following modifications.\n",
    "\n",
    "```python\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        ...\n",
    "        # change states full connected layer size to 2 agents concated\n",
    "        self.fc1 = nn.Linear(state_size*2, fc1_units)\n",
    "        # batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        ...\n",
    "```\n",
    "```python\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        ...\n",
    "        # change states full connected layer size to 2 agents concated\n",
    "        self.fcs1 = nn.Linear(state_size*2, fcs1_units)\n",
    "        # batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "        # change action layer size to 2 agents concated\n",
    "        self.fc2 = nn.Linear(fcs1_units+(action_size*2), fc2_units)\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Prioritized Replay\n",
    "\n",
    "Prioritized replay can make learning from experience replay\n",
    "more efficient. Here's one possible implementation of prioritized replay.\n",
    "\n",
    "<img src=\"./pic/priority.png\"  height=\"500\" width=\"800\" align='left'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic PER codes are from https://github.com/rlcode/per as reference with following modifications.\n",
    "```python\n",
    "class PEReplayBuffer:\n",
    "    \"\"\"\n",
    "    prioritized experience replay memory\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        ...\n",
    "        # add namedtuple to organize our experience\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        ...\n",
    "        # add initial p for adding experience \n",
    "        self.p_max = 1\n",
    "\n",
    "```\n",
    "For calc td when adding samples are quite time consuming, in my codes, I update the step experience td-error with a local saved maximum temp variable.\n",
    "```python\n",
    "    def update(self, idx, error):\n",
    "        ...\n",
    "        if self.p_max == 1:\n",
    "            self.p_max = np.max(p)\n",
    "        else:\n",
    "            if self.p_max < np.max(p):\n",
    "                self.p_max = np.max(p)\n",
    "        ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "def train(n_episodes=500, max_t=1000, random_seed=1, agent=None, debug=False):\n",
    "\n",
    "    agents = [Agent(state_size=state_size, action_size=action_size, \n",
    "                      random_seed=random_seed, \n",
    "                   num_agents=num_agents) for i in range(num_agents)]\n",
    "       \n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    scores_plot = []\n",
    "    scores_ave = []\n",
    "    scores_agent = []\n",
    "   \n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = np.reshape(env_info.vector_observations, (1,num_agents*state_size))\n",
    "        scores = np.zeros(num_agents)\n",
    "        for agent in agents:\n",
    "            agent.reset()\n",
    "\n",
    "        time_start = time.time()\n",
    "        \n",
    "        for _ in range(max_t):\n",
    "            actions = [agent.act(states, True) for agent in agents]\n",
    "            actions = np.concatenate(actions, axis=0).flatten()\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = np.reshape(env_info.vector_observations, (1, num_agents*state_size))\n",
    "            rewards = env_info.rewards  # get the reward\n",
    "            dones = env_info.local_done  # see if episode has finished\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.step(states, actions, rewards[i], next_states, dones[i], i)\n",
    "\n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "                            \n",
    "            if np.any(dones):\n",
    "                break\n",
    "            \n",
    "        duration = time.time() - time_start\n",
    "        \n",
    "        scores_window.append(np.mean(scores))  # save most recent score\n",
    "        scores_plot.append(np.mean(scores))\n",
    "        scores_ave.append(np.mean(scores_window))\n",
    "        scores_agent.append(scores)\n",
    "                \n",
    "        print('\\rEpisode {}({}sec)\\t MIN:{:.2f} MAX:{:.2f} MEAN:{:.2f} MEANo100:{:.2f} {}'.format(i_episode, \n",
    "                                    round(duration), np.min(scores), \n",
    "                                     np.max(scores), np.mean(scores), \n",
    "                                     np.mean(scores_window), ' '*10), end='')\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\nEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))            \n",
    "            # plot the scores\n",
    "            fig, ax = plt.subplots()\n",
    "            \n",
    "            plt.plot(np.arange(len(scores_ave)), scores_ave, label='Score Mean 100')\n",
    "            for i in range(num_agents):\n",
    "                plt.plot(np.arange(len(np.vstack(scores_agent))), \n",
    "                         np.vstack(scores_agent)[:,i], label='Agent {}'.format(i+1))\n",
    "            plt.plot(np.arange(len(scores_plot)), scores_plot, label='Score Ave')\n",
    "            plt.xlabel('Episode #')\n",
    "            ax.legend()\n",
    "            plt.show()\n",
    "                        \n",
    "            for i in range(num_agents):\n",
    "                torch.save(agents[i].actor_local.state_dict(), 'actor{}_{}.pth'.format(i, i_episode))\n",
    "                torch.save(agents[i].critic_local.state_dict(), 'critic{}_{}.pth'.format(i, i_episode))\n",
    "\n",
    "        if np.mean(scores_window) >= 0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode,\n",
    "                                                                                         np.mean(scores_window)))\n",
    "            for i in range(num_agents):\n",
    "                torch.save(agents[i].actor_local.state_dict(), 'actor{}.pth'.format(i))\n",
    "                torch.save(agents[i].critic_local.state_dict(), 'critic{}.pth'.format(i))\n",
    "            break\n",
    "            \n",
    "    return scores_ave, agent\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training and result\n",
    "\n",
    "I made two versions of solution.\n",
    "\n",
    "- maddpg with replay buffer. full codes(maddpg.py) [here](maddpg.py).\n",
    "- maddpg with prioritized experience replay. full codes(maddpg_v3.py) [here](maddpg_v3.py).\n",
    "\n",
    "The hyper-parameters are the same.\n",
    "\n",
    "```python\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 512        # minibatch size\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "LEARN_EVERY = 1         # learning timestep interval\n",
    "LEARN_NUM = 5           # number of learning passes\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 8e-3              # for soft update of target parameters\n",
    "OU_SIGMA = 0.2          # Ornstein-Uhlenbeck noise parameter, volatility\n",
    "OU_THETA = 0.15         # Ornstein-Uhlenbeck noise parameter, speed of mean reversion\n",
    "EPS_START = 5.0         # initial value for epsilon in noise decay process in Agent.act()\n",
    "EPS_DECAY = 6e-4        # episode to end the noise decay process\n",
    "EPS_FINAL = 0           # final value for epsilon after decay\n",
    "```\n",
    "\n",
    "#### maddpg with replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Using following codes to load the models and operation codes.\n",
    "\n",
    "\n",
    "```python\n",
    "import maddpg\n",
    "from imp import reload\n",
    "reload(maddpg)\n",
    "from maddpg import *\n",
    "```\n",
    "\n",
    "The printed result text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episode 100(41sec)\t MIN:0.09 MAX:0.20 MEAN:0.15 MEANo100:0.01            \n",
    "Episode 100\tAverage Score: 0.01\n",
    "\n",
    "Episode 200(18sec)\t MIN:-0.01 MAX:0.10 MEAN:0.05 MEANo100:0.02            \n",
    "Episode 200\tAverage Score: 0.02\n",
    "\n",
    "Episode 300(9sec)\t MIN:-0.01 MAX:0.00 MEAN:-0.00 MEANo100:0.03            \n",
    "Episode 300\tAverage Score: 0.03\n",
    "\n",
    "Episode 400(39sec)\t MIN:0.09 MAX:0.20 MEAN:0.15 MEANo100:0.03             \n",
    "Episode 400\tAverage Score: 0.03\n",
    "\n",
    "Episode 500(18sec)\t MIN:0.00 MAX:0.09 MEAN:0.05 MEANo100:0.04             \n",
    "Episode 500\tAverage Score: 0.04\n",
    "\n",
    "Episode 600(41sec)\t MIN:0.09 MAX:0.20 MEAN:0.15 MEANo100:0.05             \n",
    "Episode 600\tAverage Score: 0.05\n",
    "\n",
    "Episode 700(9sec)\t MIN:-0.01 MAX:0.00 MEAN:-0.00 MEANo100:0.03            \n",
    "Episode 700\tAverage Score: 0.03\n",
    "\n",
    "Episode 800(33sec)\t MIN:0.09 MAX:0.10 MEAN:0.10 MEANo100:0.04             \n",
    "Episode 800\tAverage Score: 0.04\n",
    "\n",
    "Episode 900(9sec)\t MIN:-0.01 MAX:0.00 MEAN:-0.00 MEANo100:0.03            \n",
    "Episode 900\tAverage Score: 0.03\n",
    "\n",
    "Episode 1000(9sec)\t MIN:-0.01 MAX:0.00 MEAN:-0.00 MEANo100:0.04           \n",
    "Episode 1000\tAverage Score: 0.04\n",
    "\n",
    "Episode 1100(18sec)\t MIN:-0.01 MAX:0.10 MEAN:0.05 MEANo100:0.04            \n",
    "Episode 1100\tAverage Score: 0.04\n",
    "\n",
    "Episode 1200(8sec)\t MIN:-0.01 MAX:0.00 MEAN:-0.00 MEANo100:0.04            \n",
    "Episode 1200\tAverage Score: 0.04\n",
    "\n",
    "Episode 1300(32sec)\t MIN:0.09 MAX:0.10 MEAN:0.10 MEANo100:0.06             \n",
    "Episode 1300\tAverage Score: 0.06\n",
    "\n",
    "Episode 1400(19sec)\t MIN:-0.01 MAX:0.10 MEAN:0.05 MEANo100:0.07            \n",
    "Episode 1400\tAverage Score: 0.07\n",
    "\n",
    "Episode 1500(8sec)\t MIN:-0.01 MAX:0.00 MEAN:-0.00 MEANo100:0.08            \n",
    "Episode 1500\tAverage Score: 0.08\n",
    "\n",
    "Episode 1600(53sec)\t MIN:0.19 MAX:0.20 MEAN:0.20 MEANo100:0.09             \n",
    "Episode 1600\tAverage Score: 0.09\n",
    "\n",
    "Episode 1700(110sec)\t MIN:0.39 MAX:0.50 MEAN:0.45 MEANo100:0.14           \n",
    "Episode 1700\tAverage Score: 0.14\n",
    "\n",
    "Episode 1800(88sec)\t MIN:0.29 MAX:0.40 MEAN:0.35 MEANo100:0.25            \n",
    "Episode 1800\tAverage Score: 0.25\n",
    "\n",
    "Episode 1900(440sec)\t MIN:1.89 MAX:1.90 MEAN:1.90 MEANo100:0.39            \n",
    "Episode 1900\tAverage Score: 0.39\n",
    "\n",
    "Episode 2000(89sec)\t MIN:0.29 MAX:0.40 MEAN:0.35 MEANo100:0.31            \n",
    "Episode 2000\tAverage Score: 0.31\n",
    "\n",
    "Episode 2100(39sec)\t MIN:0.10 MAX:0.19 MEAN:0.15 MEANo100:0.35            \n",
    "Episode 2100\tAverage Score: 0.35\n",
    "\n",
    "Episode 2200(96sec)\t MIN:0.29 MAX:0.40 MEAN:0.35 MEANo100:0.38            \n",
    "Episode 2200\tAverage Score: 0.38\n",
    "\n",
    "Episode 2300(51sec)\t MIN:0.19 MAX:0.20 MEAN:0.20 MEANo100:0.35            \n",
    "Episode 2300\tAverage Score: 0.35\n",
    "\n",
    "Episode 2400(142sec)\t MIN:0.49 MAX:0.50 MEAN:0.50 MEANo100:0.27           \n",
    "Episode 2400\tAverage Score: 0.27\n",
    "\n",
    "Episode 2500(19sec)\t MIN:-0.01 MAX:0.10 MEAN:0.05 MEANo100:0.19           \n",
    "Episode 2500\tAverage Score: 0.19\n",
    "\n",
    "Episode 2600(32sec)\t MIN:-0.01 MAX:0.10 MEAN:0.05 MEANo100:0.17           \n",
    "Episode 2600\tAverage Score: 0.17\n",
    "\n",
    "Episode 2700(151sec)\t MIN:0.59 MAX:0.60 MEAN:0.60 MEANo100:0.39           \n",
    "Episode 2700\tAverage Score: 0.39\n",
    "\n",
    "Episode 2755(33sec)\t MIN:0.09 MAX:0.10 MEAN:0.10 MEANo100:0.50            \n",
    "Environment solved in 2755 episodes!\tAverage Score: 0.50\n",
    "\n",
    "Plot score mean of 100\n",
    "\n",
    "<img src=\"./pic/100mean_maddpg.png\"  height=\"300\" width=\"400\" align='left'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details [here](./Tennis-MADDPG.ipynb).\n",
    "\n",
    "#### Watch two smart agents play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import maddpg\n",
    "from imp import reload\n",
    "reload(maddpg)\n",
    "from maddpg import *\n",
    "\n",
    "def play(play_agent, t=10, add_noise=False):\n",
    "    # trained model\n",
    "    # play times\n",
    "    for i in range(t):                                      # play game for 5 episodes\n",
    "        env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "        states = np.reshape(env_info.vector_observations, (1,num_agents*state_size))\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            actions = [a.act(states, add_noise) for a in play_agent]\n",
    "            actions = np.concatenate(actions, axis=0).flatten()\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = np.reshape(env_info.vector_observations, (1, num_agents*state_size))\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        print('Score (sum over agents) from episode {}: {:.2f}'.format(i, np.sum(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (sum over agents) from episode 0: 0.29\n",
      "Score (sum over agents) from episode 1: 0.29\n",
      "Score (sum over agents) from episode 2: 1.29\n",
      "Score (sum over agents) from episode 3: 1.39\n",
      "Score (sum over agents) from episode 4: 0.29\n",
      "Score (sum over agents) from episode 5: 0.29\n",
      "Score (sum over agents) from episode 6: 0.79\n",
      "Score (sum over agents) from episode 7: 0.19\n",
      "Score (sum over agents) from episode 8: 1.19\n",
      "Score (sum over agents) from episode 9: 0.19\n",
      "Score (sum over agents) from episode 10: 2.29\n",
      "Score (sum over agents) from episode 11: 1.39\n",
      "Score (sum over agents) from episode 12: 2.09\n",
      "Score (sum over agents) from episode 13: 0.29\n",
      "Score (sum over agents) from episode 14: 0.19\n",
      "Score (sum over agents) from episode 15: 1.09\n",
      "Score (sum over agents) from episode 16: 1.09\n",
      "Score (sum over agents) from episode 17: 0.89\n",
      "Score (sum over agents) from episode 18: 0.19\n",
      "Score (sum over agents) from episode 19: 0.39\n"
     ]
    }
   ],
   "source": [
    "# read model weights\n",
    "best_agent_0 = Agent(state_size=state_size, action_size=action_size, random_seed=1, num_agents=num_agents)\n",
    "best_agent_1 = Agent(state_size=state_size, action_size=action_size, random_seed=1, num_agents=num_agents)\n",
    "actor_state_dict_0 = torch.load('./best_pth/1st/actor0.pth', map_location='cpu')\n",
    "actor_state_dict_1 = torch.load('./best_pth/1st/actor1.pth', map_location='cpu')\n",
    "\n",
    "best_agent_0.actor_local.load_state_dict(actor_state_dict_0)\n",
    "best_agent_1.actor_local.load_state_dict(actor_state_dict_1)\n",
    "\n",
    "play([best_agent_0, best_agent_1], t=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### maddpg with prioritized experience replay\n",
    "\n",
    "Using following codes to load the models and operation codes.\n",
    "\n",
    "\n",
    "```python\n",
    "import maddpg_v3\n",
    "from imp import reload\n",
    "reload(maddpg_v3)\n",
    "from maddpg_v3 import *\n",
    "```\n",
    "\n",
    "The printed result text:\n",
    "\n",
    "Episode 100(27sec)\t MIN:-0.01 MAX:0.10 MEAN:0.05 MEANo100:0.01           \n",
    "Episode 100\tAverage Score: 0.01\n",
    "\n",
    "Episode 200(61sec)\t MIN:0.09 MAX:0.20 MEAN:0.15 MEANo100:0.05             \n",
    "Episode 200\tAverage Score: 0.05\n",
    "\n",
    "Episode 300(12sec)\t MIN:-0.01 MAX:0.00 MEAN:-0.00 MEANo100:0.04           \n",
    "Episode 300\tAverage Score: 0.04\n",
    "\n",
    "Episode 400(77sec)\t MIN:0.19 MAX:0.20 MEAN:0.20 MEANo100:0.04             \n",
    "Episode 400\tAverage Score: 0.04\n",
    "\n",
    "Episode 500(78sec)\t MIN:0.19 MAX:0.20 MEAN:0.20 MEANo100:0.04             \n",
    "Episode 500\tAverage Score: 0.04\n",
    "\n",
    "Episode 600(27sec)\t MIN:0.00 MAX:0.09 MEAN:0.05 MEANo100:0.04             \n",
    "Episode 600\tAverage Score: 0.04\n",
    "\n",
    "Episode 700(12sec)\t MIN:-0.01 MAX:0.00 MEAN:-0.00 MEANo100:0.05           \n",
    "Episode 700\tAverage Score: 0.05\n",
    "\n",
    "Episode 800(12sec)\t MIN:-0.01 MAX:0.00 MEAN:-0.00 MEANo100:0.04           \n",
    "Episode 800\tAverage Score: 0.04\n",
    "\n",
    "Episode 900(12sec)\t MIN:-0.01 MAX:0.00 MEAN:-0.00 MEANo100:0.05           \n",
    "Episode 900\tAverage Score: 0.05\n",
    "\n",
    "Episode 1000(45sec)\t MIN:0.09 MAX:0.10 MEAN:0.10 MEANo100:0.06            \n",
    "Episode 1000\tAverage Score: 0.06\n",
    "\n",
    "Episode 1100(27sec)\t MIN:-0.01 MAX:0.10 MEAN:0.05 MEANo100:0.05            \n",
    "Episode 1100\tAverage Score: 0.05\n",
    "\n",
    "Episode 1150(79sec)\t MIN:0.19 MAX:0.20 MEAN:0.20 MEANo100:0.07        \n",
    "\n",
    "...\n",
    "\n",
    "Due to the gpu time limitation, I don't have chance to get the result. But with part of epsiode result of PER, agents are more efficient of learning from experiences. Under same hyper-parameters, for 200 episodes, per agents can reach an average of 0.05 while replay buffer agents need over 600 episodes. And reach 0.07 before 1200 episodes while old agents need 1400 episodes.\n",
    "\n",
    "More details [here](./Tennis-MADDPG-PER.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Future work\n",
    "\n",
    "Implement variant ways to solve multi-agents problem to see differences among those algorithms.\n",
    "\n",
    "- Asynchronous Actor-Critic Agents (A3C)\n",
    "- Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO)\n",
    "\n",
    "### 6. Reference\n",
    "\n",
    "- Deterministic Policy Gradient Algorithms, Silver et al. 2014\n",
    "- Continuous Control With Deep Reinforcement Learning, Lillicrap et al. 2016\n",
    "- https://arxiv.org/abs/1511.05952\n",
    "- https://papers.nips.cc/paper/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments.pdf\n",
    "- https://spinningup.openai.com/en/latest/algorithms/ddpg.html#background\n",
    "- https://github.com/rlcode/per\n",
    "- https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum\n",
    "- https://github.com/shariqiqbal2810/maddpg-pytorch\n",
    "- https://towardsdatascience.com/training-two-agents-to-play-tennis-8285ebfaec5f\n",
    "- https://github.com/xuehy/pytorch-maddpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
